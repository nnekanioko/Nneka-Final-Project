# -*- coding: utf-8 -*-
"""Sentiment analysis on all combined election data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SPZA6Sgqs7C9fIC87Iw6SfSNQrvwARcA

## Sentiment Analysis for Combined Election Data
"""

import pandas as pd
import re
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import nltk
nltk.download('stopwords')

#from nltk.stem import PorterStemmer
import nltk.corpus
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

import string
import matplotlib.pyplot as plt


!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

nltk.download('punkt')
nltk.download('wordnet')

import nltk
nltk.download('omw-1.4')

from wordcloud import WordCloud, STOPWORDS
import numpy as np
from PIL import Image

df_Peter = pd.read_excel('PeterObi.xlsx', names=['hashtags', 'text','user', 'user_location','source_device', 'user_created_at','user_followers_count','user_following_count','user_verified','tweet_place','tweet_geo','tweet_created_at'])

df_Atiku = pd.read_excel('AtikuAbubakar.xlsx', names=['hashtags', 'text','user', 'user_location','source_device', 'user_created_at','user_followers_count','user_following_count','user_verified','tweet_place','tweet_geo','tweet_created_at'])

df_Tinubu = pd.read_excel('BolaTinubu.xlsx', names=['hashtags', 'text','user', 'user_location','source_device', 'user_created_at','user_followers_count','user_following_count','user_verified','tweet_place','tweet_geo','tweet_created_at'])

df = pd.concat([df_Atiku, df_Peter, df_Tinubu], names=['hashtags', 'text','user', 'user_location','source_device', 'user_created_at','user_followers_count','user_following_count','user_verified','tweet_place','tweet_geo','tweet_created_at'])

df

textcombined = df['text'].dropna()

textcombined

df_text = textcombined.to_frame
gh = textcombined.to_frame()

gh

#Data Preprocessing

def clean_textC(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('rt', '', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub(r'[ðŸâ€]', '', text)
    text = re.sub(r'[â€™â€¦]', '', text)
    text = re.sub('tâ', '', text)
    text = re.sub('x9', '', text)
    text = re.sub('ÿ', '', text)
    text = re.sub(r'[â€™]', '', text)
    text = re.sub(r'[œðŸ]', '', text)
    text = re.sub(r'[ðŸ‘‡ðŸ‘‡ðŸ¤ŒðŸ“¢ðŸ•Šï]', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = re.sub(r"@([a-zA-Z0-9_]{1,50})",'',str(text))

    return text

#Text Preprocessing

def Text_Processing(Text):
  Processed_Text = list()
  Lemmatizer = WordNetLemmatizer()

 
 

  # Tokens of Words
  Tokens = nltk.word_tokenize(Text)


  # Removing Stopwords and Lemmatizing Words
  # To reduce noises in our dataset, also to keep it simple and still 
  # powerful, we will only omit the word `not` from the list of stopwords

  

  for word in Tokens:
    if word not in stop_words:
      Processed_Text.append(Lemmatizer.lemmatize(word))

 

 

  return(" ".join(Processed_Text))

 

 

  return Text


#applying function to data
gh["text"]= gh["text"].apply(lambda text: clean_textC(text))

gh

gh["text"].head(15)

gh["text"]= gh["text"].apply(lambda Text: Text_Processing(Text))



gh["text"]

analyser = SentimentIntensityAnalyzer()
def sentiment_analyzer_scores(sentence):
    score = analyser.polarity_scores(sentence)
    return score["compound"]

gh["polarity"]=gh["text"].apply(sentiment_analyzer_scores)

def sentiment_col(y):
    if y >= 0.05 :
        return 'positive'
    elif y > -0.05 <0.05 :
        return 'neutral'
    elif y <= 0.05:
        return 'negative'

gh['polarity'] = gh['polarity'].apply(sentiment_col)

gh['polarity']

"""Plot PieChart of Sentiments """

polCom=gh['polarity'].groupby(gh['polarity']).size().reset_index(name='count')

polCom.head(15)
polCom=polCom.iloc[:60,::]
polCom

polCom=gh['polarity'].groupby(gh['polarity']).size().reset_index(name='count')

polCom.head(15)
polCom=polCom.iloc[:60,::]
polCom

polarity=polCom['polarity']
TotalPol=polCom['count']
explode=[0.1,0.1,0.1]
#colors = ['lightcoral', 'lightblue', 'lightgreen']

plt.pie(TotalPol, labels = polarity, radius=1.1,  autopct='%2.1f%%', explode=explode)
plt.title('Sentiments Analysis Using Vader Lexicon')
#plt.legend(labels)
plt.show()

import seaborn as sns
from matplotlib import style
style.use('ggplot')
import matplotlib.pyplot as plt
import random
dfC = gh.groupby('polarity').size().reset_index(name='coun')
n = dfC['polarity'].unique().__len__()+1
all_colors = list(plt.cm.colors.cnames.keys())
random.seed(1000)
c = random.choices(all_colors)

# Plot Bars
plt.figure(figsize=(10,5), dpi= 200)
plt.bar(dfC['polarity'], dfC['coun'], color=['black', 'blue', 'green'], width=.5)
for i, val in enumerate(dfC['coun'].values):
    plt.text(i, val, float(val), horizontalalignment='center', verticalalignment='bottom', fontdict={'fontweight':500, 'size':25}) 

plt.title('Sentiment Analysis for Combined Election Data')

#WORD CLOUD AND FREQUENCY DISTRIBUTION
from nltk.tokenize import word_tokenize
pos_tweet = gh[gh.polarity == 'positive']
text = ' '.join([word for word in pos_tweet['text']])
corpus = text

sentences = nltk.sent_tokenize(corpus)

print(type(sentences))

sentence_tokens = ""
for sentence in sentences:
    sentence_tokens += sentence
    


#word tokenization

words=nltk.word_tokenize(sentence_tokens)
print(words)
for word in words:
    print(word)
    

from nltk.corpus import stopwords
print(stopwords.words('english'))

#stop word removal
stop_words = set(stopwords.words('english'))
filtered_words=[]

for w in words:
    if w not in stop_words:
        filtered_words.append(w)

print('/n With stop words:', words)
print('/n After removing stop words:', filtered_words)


#finding the frequency distribution of words
frequency_dist = nltk.FreqDist(filtered_words)

#SORTING THE FREQUENCY DISTRIBUTION
sorted(frequency_dist,key=frequency_dist.__getitem__,reverse=True)[0:30]

#Keeping only the large words(more than 3 characters)
large_words = dict([(k,v) for k,v in frequency_dist.items() if len (k)>4])

frequency_dist = nltk.FreqDist(large_words)
frequency_dist.plot(30, cumulative=False)

#Visualising the distribution of words using matplotlib and wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt


wordcloud = WordCloud(max_font_size=50, max_words=100,background_color="white").generate_from_frequencies(frequency_dist)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Visualising the distribution of words using matplotlib and wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt


wordcloud = WordCloud(max_font_size=50, max_words=100,background_color="white").generate_from_frequencies(frequency_dist)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""WORD CLOUD AND FREQUENCY DISTRIBUTION FOR NEGATIVE SENTIMENT"""

#WORD CLOUD AND FREQUENCY DISTRIBUTION
from nltk.tokenize import word_tokenize
neg_tweet = gh[gh.polarity == 'negative']
text = ' '.join([word for word in neg_tweet['text']])
corpus = text

sentences = nltk.sent_tokenize(corpus)

print(type(sentences))

sentence_tokens = ""
for sentence in sentences:
    sentence_tokens += sentence
    


#word tokenization

words=nltk.word_tokenize(sentence_tokens)
print(words)
for word in words:
    print(word)
    

from nltk.corpus import stopwords
print(stopwords.words('english'))

#stop word removal
stop_words = set(stopwords.words('english'))
filtered_words=[]

for w in words:
    if w not in stop_words:
        filtered_words.append(w)

print('/n With stop words:', words)
print('/n After removing stop words:', filtered_words)


#finding the frequency distribution of words
frequency_dist = nltk.FreqDist(filtered_words)

#SORTING THE FREQUENCY DISTRIBUTION
sorted(frequency_dist,key=frequency_dist.__getitem__,reverse=True)[0:30]

#Keeping only the large words(more than 3 characters)
large_words = dict([(k,v) for k,v in frequency_dist.items() if len (k)>4])

frequency_dist = nltk.FreqDist(large_words)
frequency_dist.plot(30, cumulative=False)

#Visualising the distribution of words using matplotlib and wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt


wordcloud = WordCloud(max_font_size=50, max_words=100,background_color="black").generate_from_frequencies(frequency_dist)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""#**APPLYING MACHINE LEARNING TO TWEETS**"""

from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten
#from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
 

#from nltk.stem import PorterStemmer
import nltk.corpus
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

from sklearn.metrics import accuracy_score 
from sklearn.feature_extraction.text import TfidfVectorizer

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Convert the text data into a matrix of TF-IDF features
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(gh["text"])
 
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X,gh['polarity'], test_size=0.2)

"""#Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import KFold
import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split


# load your twitter dataset
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(gh["text"])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X,gh['polarity'], test_size=0.2)

# Train a naive Bayes classifier
clf = MultinomialNB()
clf.fit(X_train,y_train)

# Make predictions on the testing set
predictions = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: ", accuracy)

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.naive_bayes import MultinomialNB


# Calculate accuracy
accuracyNB = np.mean(predictions == y_test)
print("Accuracy:", accuracyNB)

# Calculate AUC
try:
    from sklearn.metrics import roc_auc_score
    auc = roc_auc_score(y_test, predictions)
    print("AUC:", auc)
except:
    print("AUC is not applicable for this problem")

# Calculate precision, recall, and F-Measure
print(classification_report(y_test, predictions))

from sklearn.metrics import confusion_matrix

# calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predictions)

# plot the confusion matrix
import seaborn as sns
sns.heatmap(conf_mat, annot=True)
plt.imshow(conf_mat, cmap='binary')
plt.show()

conf_mat

"""#Support Vector Machine"""

#SVM
# Train a linear SVM classifier
clf = LinearSVC()
clf.fit(X_train, y_train)

# Make predictions on the testing set
predictions = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: ", accuracy)

train_text, test_text, train_cat, test_cat = train_test_split(gh['text'],gh['polarity'], test_size = 0.20, random_state = 42)

from sklearn.metrics import confusion_matrix

# calculate the confusion matrix
conf_mat = confusion_matrix(y_test, predictions)

# plot the confusion matrix
import seaborn as sns
sns.heatmap(conf_mat, annot=True)
plt.imshow(conf_mat, cmap='binary')
plt.show()

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.svm import SVC

# Calculate accuracy
accuracy = np.mean(predictions == y_test)
print("Accuracy:", accuracy)

# Calculate AUC
try:
    from sklearn.metrics import roc_auc_score
    auc = roc_auc_score(y_test, predictions)
    print("AUC:", auc)
except:
    print("AUC is not applicable for this problem")

# Calculate precision, recall, and F-Measure
print(classification_report(y_test, predictions))

#Machine Learning classification
# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 1744
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 254
# This is fixed.
EMBEDDING_DIM = 100
#Tokenization for deep learning
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(gh['text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

#Truncate and pad the input sequences so that they are all in the same length for modeling
X = tokenizer.texts_to_sequences(gh['text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

 

#Converting categorical labels to numbers
Y = pd.get_dummies(gh['polarity']).values
print('Shape of label tensor:', Y.shape)

#Splitting of data to 80% training and 20% testing
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

"""#LSTM MODEL

"""

#lstm model
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

 

epochs = 10
batch_size = 64

 

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])
accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

 

#test new data with new complaint
plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show();

import numpy as np


# Evaluating the model

from sklearn.metrics import confusion_matrix, classification_report


Predict = model.predict(X_test)


Y_pred = np.argmax(Predict, axis=1)
Y_true = np.argmax(Y_test,  axis=1)



#evaluation
import seaborn as sns
import matplotlib.pyplot as plt
cf_matrix = confusion_matrix(Y_true, Y_pred)
sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,
            fmt='.2%', cmap='Blues')

plt.xlabel("Predicted label")
plt.ylabel("Actual label")
plt.title('Confusion matrix')
plt.show()

cf_matrix = confusion_matrix(Y_true, Y_pred)
sns.heatmap(cf_matrix, annot=True)



print("\n Classification Report:")
target_classes = ['No event 0 (-)', 'Event 1 (+)', 'Event 2 (+)']
print(classification_report(Y_true, Y_pred, target_names=target_classes))
print(classification_report(Y_true, Y_pred))

import pandas as pd
import matplotlib.pyplot as plot
# Create DataFrame
dfmetrics = pd.DataFrame({"Naive Bayes":[89,90, 89,89 ],
                   "SVM":[97, 98, 98,97],
                   "LSTM":[98, 99, 98,98]},
                  index = ["Accuracy", "Precision", "Recall", "F1-Score"])

# Create unstacked multiple columns bar
dfmetrics.plot(kind="bar", figsize = (8, 4),  title="Sentiment Analysis Model Performance")

"""#**CREATING WORD CLOUD FOR COMBINED ELECTION TWEETS**"""

from wordcloud import WordCloud, STOPWORDS
import numpy as np
from PIL import Image

#Function to Create Wordcloud
def create_wordcloud(text):
    mask = np.array(Image.open("/content/cloud.jpg"))
    stopwords = set(STOPWORDS)
    wc = WordCloud(background_color="black",
    mask = mask,
    max_words=3000,
    stopwords=stopwords,
    repeat=True)
    wc.generate(str(text))
    wc.to_file("wc.png")
    print("Word Cloud Saved Successfully")
    path="wc.png"
    display(Image.open(path))

#Creating wordcloud for all tweets
create_wordcloud(gh["text"].values)

"""#**EXPLORATORY ANALYSIS FOR COMBINED TWEETS**"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
import json
import seaborn as sns
import re
import collections
import random

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#TIMESERIES FOR ALL COMBINED TWEETS"""

timeseries = df['tweet_created_at'].dropna()

timeseries

#PLOT NUMBER OF TWEET AGAINST TWEET CREATED DATE
df['DateNew'] = pd.to_datetime(timeseries, errors='coerce').dt.date
df['Time'] = pd.to_datetime(timeseries, errors='coerce').dt.time

df.head(5)

df = df[~df.index.duplicated()]

nc=df.groupby('DateNew').size().reset_index(name='tweet_volume')

nc.head(15)
nc=nc.iloc[:60,::]
nc

nc.plot.bar(x='DateNew', y="tweet_volume", rot=90, title="Tweet Dates for Combined Election Data")


#Using Seaborn
axc=sns.barplot(x='DateNew', y='tweet_volume', data=nc, palette='tab10')

plt.xlabel('Date Tweet Created')
plt.ylabel('Number of Tweets')
plt.show(block=True)

"""Time Series for  (Tweet Time)"""

tmC=df.groupby('Time').size().reset_index(name='tweet_volume')

tmC.head(15)
tmC=tmC.iloc[:60,::]
tmC

tmC.plot.bar(x='Time', y="tweet_volume", rot=90, title="Tweet Time for Combined Election Data")


#Using Seaborn
ax=sns.barplot(x='Time', y='tweet_volume', data=tmC, palette='spring_r')

plt.xlabel('Time Tweet Created')
plt.ylabel('Number of Tweets')
plt.show(block=True)

"""USER CREATED DATES FOR COMBINED TWEETS

#TWEET DEVICES FOR COMBINED TWEETS
"""

#TWEET DEVICES FOR ALL ELECTION DATA

# Prepare Data
def fill_column(x):
    if x == '<a href="http://twitter.com/download/iphone" rel="nofollow">Twitter for iPhone</a>':
        return 'Iphone'
    elif x== '<a href="http://twitter.com/download/android" rel="nofollow">Twitter for Android</a>':
        return 'Andrioid'
    elif x == '<a href="https://mobile.twitter.com" rel="nofollow">Twitter Web App</a>':
        return 'Web'
    else:
        return 'Others'

df['source_device']

devices = df['source_device'].dropna()

#PLOT DEVICES

df['source_device'] = df['source_device'].apply(fill_column)
df1 = df.groupby('source_device').size().reset_index(name='counts')
n = df1['source_device'].unique().__len__()+1
all_colors = list(plt.cm.colors.cnames.keys())
random.seed(100)
c = random.choices(all_colors, k=n)

dev=df.groupby(df['source_device']).size().reset_index(name='tweet_count')

dev.head(15)
dev=dev.iloc[:60,::]
dev

dev.plot.bar(x='source_device', y="tweet_count", rot=90, title="Tweet Devices for Combined Tweeets")


#Using Seaborn
ax=sns.barplot(x='source_device', y='tweet_count', data=dev, palette='tab10')

plt.xlabel('Tweet Devices')
plt.ylabel('Total Tweet Count')
plt.show(block=True)

p=sns.barplot(x='source_device', y='tweet_count', data=dev, ci=None,)
sns.set(rc = {'figure.figsize':(15,6)})

p.set(title="Tweet Devices for Combined Tweeets")

pdev=sns.barplot(x='source_device', y='tweet_count', data=dev,ci=None,order=dev.sort_values('tweet_count', ascending=False).source_device , palette='tab10')
sns.set(rc = {'figure.figsize':(15,6)})
pdev.set(title="Tweet Devices for Combined Tweeets")

devType=dev['source_device']
Total=dev['tweet_count']
explode=[0.1,0.1,0.1,0.1,]

plt.pie(Total, labels = devType,radius=1.1, autopct='%2.1f%%', explode=explode)
plt.title('Tweet Devices for Combined Tweeets')
plt.legend(devType)
plt.show()

"""#VERIFIED DEVICES FOR COMBINED TWEETS"""

#PLOTTING VERIFIED AND UNVERIFIED USERS
df.groupby(['user_verified'])['source_device'].count().plot.pie(figsize=(10,10),autopct='%1.1f%%')

vef=df.groupby(df['user_verified']).size().reset_index(name='tweet_count')

vefType=vef['user_verified']
Totalvef=vef['tweet_count']
explode=[0,0,0,0.1,0]

vef.head(15)
vef=vef.iloc[:60,::]
vef
